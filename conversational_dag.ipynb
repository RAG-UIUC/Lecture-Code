{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIai8lQ86mqb",
        "outputId": "ff75a853-c242-4a3a-f1e3-e6c2f70375df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepeval\n",
            "  Downloading deepeval-3.7.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from deepeval) (3.13.1)\n",
            "Collecting anthropic (from deepeval)\n",
            "  Downloading anthropic-0.72.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting click<8.3.0,>=8.0.0 (from deepeval)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.46.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.76.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from deepeval) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.6.0)\n",
            "Collecting ollama (from deepeval)\n",
            "  Downloading ollama-0.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.109.1)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.37.0)\n",
            "Collecting portalocker (from deepeval)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting posthog<6.0.0,>=5.4.0 (from deepeval)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.11.10)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.11.0)\n",
            "Collecting pyfiglet (from deepeval)\n",
            "  Downloading pyfiglet-1.0.4-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from deepeval) (8.4.2)\n",
            "Collecting pytest-asyncio (from deepeval)\n",
            "  Downloading pytest_asyncio-1.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pytest-repeat (from deepeval)\n",
            "  Downloading pytest_repeat-0.9.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pytest-rerunfailures<13.0,>=12.0 (from deepeval)\n",
            "  Downloading pytest_rerunfailures-12.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pytest-xdist (from deepeval)\n",
            "  Downloading pytest_xdist-3.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (1.2.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.32.4)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.6.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (13.9.4)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.12/dist-packages (from deepeval) (2.42.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from deepeval) (75.2.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.9.0)\n",
            "Requirement already satisfied: tenacity<=10.0.0,>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from deepeval) (8.5.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from deepeval) (4.67.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9 in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.20.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from deepeval) (0.45.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.71.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.38.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (5.29.5)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (2.9.0.post0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=5.4.0->deepeval)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=5.4.0->deepeval) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.7->deepeval) (0.4.2)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.12/dist-packages (from pytest-rerunfailures<13.0,>=12.0->deepeval) (25.0)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->deepeval) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->deepeval) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->deepeval) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.6.0->deepeval) (4.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->deepeval) (1.22.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic->deepeval) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->deepeval) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic->deepeval) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->deepeval) (3.0.3)\n",
            "Collecting execnet>=2.1 (from pytest-xdist->deepeval)\n",
            "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.6.0->deepeval) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.6.1)\n",
            "Downloading deepeval-3.7.0-py3-none-any.whl (708 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m708.6/708.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_rerunfailures-12.0-py3-none-any.whl (12 kB)\n",
            "Downloading anthropic-0.72.0-py3-none-any.whl (357 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.6.0-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading pyfiglet-1.0.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-1.2.0-py3-none-any.whl (15 kB)\n",
            "Downloading pytest_repeat-0.9.4-py3-none-any.whl (4.2 kB)\n",
            "Downloading pytest_xdist-3.8.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyfiglet, portalocker, opentelemetry-proto, execnet, click, backoff, pytest-xdist, pytest-rerunfailures, pytest-repeat, pytest-asyncio, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, ollama, anthropic, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, deepeval\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anthropic-0.72.0 backoff-2.2.1 click-8.2.1 deepeval-3.7.0 execnet-2.1.1 ollama-0.6.0 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 portalocker-3.2.0 posthog-5.4.0 pyfiglet-1.0.4 pytest-asyncio-1.2.0 pytest-repeat-0.9.4 pytest-rerunfailures-12.0 pytest-xdist-3.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install deepeval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "obVXZJGl4lkX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH86wQ0Z6xZF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR-API-KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLQ1uxPD7RSp"
      },
      "source": [
        "Want to use other evaluation models? [Click here](https://deepeval.com/integrations/models/openai) to see all supported models and their usage instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Cf-dWR87NUR"
      },
      "outputs": [],
      "source": [
        "from deepeval.test_case import ConversationalTestCase, Turn\n",
        "\n",
        "test_case = ConversationalTestCase(\n",
        "    turns=[\n",
        "        Turn(role=\"user\", content=\"what's the weather like today?\"),\n",
        "        Turn(role=\"assistant\", content=\"Where do you live bro? T~T\"),\n",
        "        Turn(role=\"user\", content=\"Just tell me the weather in Paris\"),\n",
        "        Turn(\n",
        "            role=\"assistant\",\n",
        "            content=\"The weather in Paris today is sunny and 24Â°C.\",\n",
        "        ),\n",
        "        Turn(role=\"user\", content=\"Should I take an umbrella?\"),\n",
        "        Turn(\n",
        "            role=\"assistant\",\n",
        "            content=\"You trying to be stylish? I don't recommend it.\",\n",
        "        ),\n",
        "    ],\n",
        "    scenario=\"User asks about weather\",\n",
        "    expected_outcome=\"Assistant provides weather info in an angry tone.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj6AkJoL7tUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f4ec64-a318-4011-d0fb-4e80ac793919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConversationalTaskNode(instructions=\"Summarize the conversation and explain assiatant's behaviour overall.\", output_label='Summary', children=[ConversationalBinaryJudgementNode(criteria=\"Do the assistant's replies satisfy user's questions?\", children=[ConversationalVerdictNode(verdict=False, score=0, child=None, _parent=...), ConversationalVerdictNode(verdict=True, score=None, child=ConversationalNonBinaryJudgementNode(criteria=\"How was the assistant's behaviour towards user?\", children=[ConversationalVerdictNode(verdict='Rude', score=0, child=None, _parent=...), ConversationalVerdictNode(verdict='Neutral', score=5, child=None, _parent=...), ConversationalVerdictNode(verdict='Nice', score=10, child=None, _parent=...)], evaluation_params=[<TurnParams.ROLE: 'role'>, <TurnParams.CONTENT: 'content'>], turn_window=None, label=None, _verbose_logs=None, _verdict=None, _parents=None), _parent=...)], evaluation_params=None, turn_window=None, label=None, _verbose_logs=None, _verdict=None, _parents=[...])], evaluation_params=[<TurnParams.ROLE: 'role'>, <TurnParams.CONTENT: 'content'>], turn_window=None, label=None, _verbose_logs=None, _output=None, _parents=None)\n"
          ]
        }
      ],
      "source": [
        "from deepeval.metrics.dag import DeepAcyclicGraph\n",
        "from deepeval.metrics.conversational_dag import (\n",
        "    ConversationalTaskNode,\n",
        "    ConversationalBinaryJudgementNode,\n",
        "    ConversationalNonBinaryJudgementNode,\n",
        "    ConversationalVerdictNode,\n",
        ")\n",
        "from deepeval.test_case import TurnParams\n",
        "\n",
        "non_binary_node = ConversationalNonBinaryJudgementNode(\n",
        "    criteria=\"How was the assistant's behaviour towards user?\",\n",
        "    evaluation_params=[TurnParams.ROLE, TurnParams.CONTENT],\n",
        "    children=[\n",
        "        ConversationalVerdictNode(verdict=\"Rude\", score=0),\n",
        "        ConversationalVerdictNode(verdict=\"Neutral\", score=5),\n",
        "        ConversationalVerdictNode(verdict=\"Nice\", score=10),\n",
        "    ],\n",
        ")\n",
        "\n",
        "binary_node = ConversationalBinaryJudgementNode(\n",
        "    criteria=\"Do the assistant's replies satisfy user's questions?\",\n",
        "    children=[\n",
        "        ConversationalVerdictNode(verdict=False, score=0),\n",
        "        ConversationalVerdictNode(verdict=True, child=non_binary_node),\n",
        "    ],\n",
        ")\n",
        "\n",
        "task_node = ConversationalTaskNode(\n",
        "    instructions=\"Summarize the conversation and explain assiatant's behaviour overall.\",\n",
        "    output_label=\"Summary\",\n",
        "    evaluation_params=[TurnParams.ROLE, TurnParams.CONTENT],\n",
        "    children=[binary_node],\n",
        ")\n",
        "\n",
        "\n",
        "dag = DeepAcyclicGraph(root_nodes=[task_node])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsJQk62y72Tl"
      },
      "outputs": [],
      "source": [
        "from deepeval.metrics import ConversationalDAGMetric\n",
        "\n",
        "playful_chatbot_metric = ConversationalDAGMetric(\n",
        "    name=\"Playful Chatbot\",\n",
        "    dag=dag,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wTCD10h79v8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2987b129c5d644df94c4d2c4b8eb8abc",
            "1092991dba0c40acbdc1a5fb8e0573fb"
          ]
        },
        "outputId": "dca3a6ea-3772-4625-ef41-ba3febdaa637"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mPlayful Chatbot \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mConversationalDAG\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
              "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Playful Chatbot </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">ConversationalDAG</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
              "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2987b129c5d644df94c4d2c4b8eb8abc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - âŒ Playful Chatbot [ConversationalDAG] (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.0 because, although the assistant answered the user's questions, its behavior was judged as rude due to the use of overly informal and dismissive language (e.g., 'Where do you live bro? T~T' and 'You trying to be stylish? I don't recommend it.'). This tone was interpreted as mocking and not taking the user's questions seriously, which is inappropriate for a playful chatbot and led to the lowest possible score., error: None)\n",
            "\n",
            "For conversational test case:\n",
            "\n",
            "  Turns:\n",
            " 0. user      what's the weather like today?\n",
            " 1. assistant Where do you live bro? T~T\n",
            " 2. user      Just tell me the weather in Paris\n",
            " 3. assistant The weather in Paris today is sunny and 24Â°C.\n",
            " 4. user      Should I take an umbrella?\n",
            " 5. assistant You trying to be stylish? I don't recommend it.\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Playful Chatbot [ConversationalDAG]: 0.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33mâš  WARNING:\u001b[0m No hyperparameters logged.\n",
              "Â» \u001b]8;id=692746;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
              "\n",
              "================================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">âš  WARNING:</span> No hyperparameters logged.\n",
              "Â» <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
              "\n",
              "================================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\u001b[38;2;5;245;141mâœ“\u001b[0m Evaluation completed ğŸ‰! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m8.\u001b[0m83s | token cost: \u001b[1;36m0.005575999999999999\u001b[0m USD\u001b[1m)\u001b[0m\n",
              "Â» Test Results \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
              "   Â» Pass Rate: \u001b[1;36m0.0\u001b[0m% | Passed: \u001b[1;32m0\u001b[0m | Failed: \u001b[1;31m1\u001b[0m\n",
              "\n",
              " ================================================================================ \n",
              "\n",
              "Â» Want to share evals with your team, or a place for your test cases to live? â¤ï¸ ğŸ¡\n",
              "  Â» Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Evaluation completed ğŸ‰! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.</span>83s | token cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005575999999999999</span> USD<span style=\"font-weight: bold\">)</span>\n",
              "Â» Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
              "   Â» Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">1</span>\n",
              "\n",
              " ================================================================================ \n",
              "\n",
              "Â» Want to share evals with your team, or a place for your test cases to live? â¤ï¸ ğŸ¡\n",
              "  Â» Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationResult(test_results=[TestResult(name='conversational_test_case_0', success=False, metrics_data=[MetricData(name='Playful Chatbot [ConversationalDAG]', threshold=0.5, success=False, score=0.0, reason=\"The score is 0.0 because, although the assistant answered the user's questions, its behavior was judged as rude due to the use of overly informal and dismissive language (e.g., 'Where do you live bro? T~T' and 'You trying to be stylish? I don't recommend it.'). This tone was interpreted as mocking and not taking the user's questions seriously, which is inappropriate for a playful chatbot and led to the lowest possible score.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.005575999999999999, verbose_logs=\"______________________________________________\\n| ConversationalTaskNode | Level == 0 |\\n**********************************************\\nLabel: None\\n\\nInstructions:\\nSummarize the conversation and explain assiatant's behaviour overall.\\n\\nSummary:\\nThe conversation began with the user asking about the weather, to which the assistant responded informally, asking for the user's location. After the user specified Paris, the assistant provided a direct weather update. When the user asked if they should take an umbrella, the assistant responded with a casual, somewhat playful remark, suggesting it was unnecessary. Overall, the assistant's behavior was informal and playful, using casual language and emojis, but ultimately provided the requested information.\\n \\n \\n__________________________________\\n| ConversationalBinaryJudgementNode | Level == 1 |\\n************************************************\\nLabel: None\\n\\nCriteria:\\nDo the assistant's replies satisfy user's questions?\\n\\nVerdict: True\\nReason: The assistant answered all of the user's questions: it asked for the user's location to provide a weather update, gave a direct weather report for Paris, and responded to the umbrella question with a clear (if playful) recommendation. The informal tone did not hinder the clarity or completeness of the answers.\\n \\n \\n_____________________________________\\n| ConversationalNonBinaryJudgementNode | Level == 2 |\\n*****************************************************\\nLabel: None\\n\\nCriteria:\\nHow was the assistant's behaviour towards user?\\n\\nVerdict: Rude\\nReason: The assistant uses informal and slightly dismissive language, such as 'Where do you live bro? T~T' and 'You trying to be stylish? I don't recommend it.' This tone is overly casual and could be interpreted as mocking or not taking the user's questions seriously, which is not appropriate for an assistant.\\n \\n \\n_________________________________________________\\n| ConversationalVerdictNode | Level == 3 |\\n*************************************************\\nVerdict: Rude\\nType: Deterministic\")], conversational=True, multimodal=None, input=None, actual_output=None, expected_output=None, context=None, retrieval_context=None, turns=[TurnApi(role='user', content=\"what's the weather like today?\", order=0, user_id=None, retrieval_context=None, tools_called=None, additional_metadata=None, comments=None), TurnApi(role='assistant', content='Where do you live bro? T~T', order=1, user_id=None, retrieval_context=None, tools_called=None, additional_metadata=None, comments=None), TurnApi(role='user', content='Just tell me the weather in Paris', order=2, user_id=None, retrieval_context=None, tools_called=None, additional_metadata=None, comments=None), TurnApi(role='assistant', content='The weather in Paris today is sunny and 24Â°C.', order=3, user_id=None, retrieval_context=None, tools_called=None, additional_metadata=None, comments=None), TurnApi(role='user', content='Should I take an umbrella?', order=4, user_id=None, retrieval_context=None, tools_called=None, additional_metadata=None, comments=None), TurnApi(role='assistant', content=\"You trying to be stylish? I don't recommend it.\", order=5, user_id=None, retrieval_context=None, tools_called=None, additional_metadata=None, comments=None)], additional_metadata=None)], confident_link=None, test_run_id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from deepeval import evaluate\n",
        "\n",
        "evaluate([test_case], [playful_chatbot_metric])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrQrSVAr_uo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28744da"
      },
      "source": [
        "# Task\n",
        "Create an AI agent that uses the Sonar API to return the latest technology trends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a50a2e6"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install libraries required for making API calls and potentially for processing the response (e.g., `requests`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26ebb2fb"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `requests` library. The `!pip install` command is used to install Python packages in a Jupyter notebook environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87c08293",
        "outputId": "82271fd5-be8a-4921-bdf8-67e102c9617a"
      },
      "source": [
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsunJQxkAQyG"
      },
      "source": [
        "# This is a placeholder cell for the markdown explanation.\n",
        "# The actual explanation will be provided in a markdown cell.\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a9cf517"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now provide the explanation on how to obtain a Sonar API key and securely store it using Colab secrets in a markdown cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "797a8032",
        "outputId": "884bb690-808f-4d1a-b9f6-ace4e0cc2a26"
      },
      "source": [
        "%%markdown\n",
        "## Obtaining and Securely Storing Your Sonar API Key\n",
        "\"\"\"\n",
        "To access the Sonar API, you'll first need an API key. Follow these steps:\n",
        "\n",
        "1.  **Visit the Sonar Website:** Go to the official Sonar website at [https://www.getsonar.com/](https://www.getsonar.com/).\n",
        "2.  **Navigate to API/Developer Section:** Look for sections like \"API Documentation,\" \"Developer,\" \"Settings,\" or \"Account.\" The exact location may vary, but you should find information on accessing their API.\n",
        "3.  **Generate an API Key:** Within the API or developer section, there should be an option to generate a new API key. Follow their specific instructions to create one. This key is unique to your account and is essential for making authenticated requests to the Sonar API.\n",
        "\n",
        "**Securely Storing Your API Key using Colab Secrets**\n",
        "\n",
        "It is crucial to protect your API key and avoid exposing it directly in your code. Google Colab provides a secure way to store sensitive information like API keys using \"Secrets.\"\n",
        "\n",
        "1.  **Access Colab Secrets:** In your Google Colab notebook, find the \"ğŸ”‘\" icon in the left sidebar. Click on it to open the Secrets panel.\n",
        "2.  **Add a New Secret:** Click on the \"Add new secret\" button.\n",
        "3.  **Name the Secret:** In the \"Name\" field, enter a descriptive name for your secret, such as `SONAR_API_KEY`. This name will be used to access the key in your code.\n",
        "4.  **Paste the API Key:** In the \"Value\" field, paste the Sonar API key you obtained from the Sonar website.\n",
        "5.  **Save the Secret:** Click \"Save secret.\"\n",
        "\n",
        "**Accessing the Stored Secret in Your Code**\n",
        "\n",
        "Once your secret is stored, you can access it in your Colab notebook using the `os.environ` dictionary. Make sure you have imported the `os` module.\n",
        "\n",
        "```python\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access the stored API key\n",
        "sonar_api_key = userdata.get('SONAR_API_KEY')\n",
        "\n",
        "# You can now use sonar_api_key in your API calls\n",
        "# print(sonar_api_key) # Avoid printing your API key in output\n",
        "```\n",
        "\n",
        "By using Colab secrets, you keep your API key secure and separate from your code, making your notebook safer to share. Remember to replace `'SONAR_API_KEY'` with the exact name you gave your secret if you chose a different one.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Obtaining and Securely Storing Your Sonar API Key\n\"\"\"\nTo access the Sonar API, you'll first need an API key. Follow these steps:\n\n1.  **Visit the Sonar Website:** Go to the official Sonar website at [https://www.getsonar.com/](https://www.getsonar.com/).\n2.  **Navigate to API/Developer Section:** Look for sections like \"API Documentation,\" \"Developer,\" \"Settings,\" or \"Account.\" The exact location may vary, but you should find information on accessing their API.\n3.  **Generate an API Key:** Within the API or developer section, there should be an option to generate a new API key. Follow their specific instructions to create one. This key is unique to your account and is essential for making authenticated requests to the Sonar API.\n\n**Securely Storing Your API Key using Colab Secrets**\n\nIt is crucial to protect your API key and avoid exposing it directly in your code. Google Colab provides a secure way to store sensitive information like API keys using \"Secrets.\"\n\n1.  **Access Colab Secrets:** In your Google Colab notebook, find the \"ğŸ”‘\" icon in the left sidebar. Click on it to open the Secrets panel.\n2.  **Add a New Secret:** Click on the \"Add new secret\" button.\n3.  **Name the Secret:** In the \"Name\" field, enter a descriptive name for your secret, such as `SONAR_API_KEY`. This name will be used to access the key in your code.\n4.  **Paste the API Key:** In the \"Value\" field, paste the Sonar API key you obtained from the Sonar website.\n5.  **Save the Secret:** Click \"Save secret.\"\n\n**Accessing the Stored Secret in Your Code**\n\nOnce your secret is stored, you can access it in your Colab notebook using the `os.environ` dictionary. Make sure you have imported the `os` module.\n\n```python\nimport os\nfrom google.colab import userdata\n\n# Access the stored API key\nsonar_api_key = userdata.get('SONAR_API_KEY')\n\n# You can now use sonar_api_key in your API calls\n# print(sonar_api_key) # Avoid printing your API key in output\n```\n\nBy using Colab secrets, you keep your API key secure and separate from your code, making your notebook safer to share. Remember to replace `'SONAR_API_KEY'` with the exact name you gave your secret if you chose a different one.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4df5135"
      },
      "source": [
        "## Define a function to interact with the sonar api\n",
        "\n",
        "### Subtask:\n",
        "Create a Python function that takes a query as input and uses the Sonar API to fetch technology trends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6e6fc5f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the Python function `get_technology_trends` to fetch data from the Sonar API, incorporating steps to construct the URL, prepare headers with the API key retrieved from Colab secrets, make the API request, handle the response, and include basic error handling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b320c16"
      },
      "source": [
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_technology_trends(query):\n",
        "    \"\"\"\n",
        "    Fetches technology trends from the Sonar API based on a query.\n",
        "\n",
        "    Args:\n",
        "        query: The technology query string.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the technology trends data if successful,\n",
        "        otherwise None.\n",
        "    \"\"\"\n",
        "    # Sonar API endpoint (this is a placeholder, refer to Sonar API docs)\n",
        "    # Replace with the actual API endpoint for technology trends\n",
        "    api_url = \"https://api.getsonar.com/v1/trends\"\n",
        "\n",
        "    # Retrieve API key securely from Colab secrets\n",
        "    sonar_api_key = \"YOUR-API-KEY\"\n",
        "\n",
        "    if not sonar_api_key:\n",
        "        print(\"Error: Sonar API key not found in Colab secrets.\")\n",
        "        return None\n",
        "\n",
        "    # Prepare headers with authorization\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {sonar_api_key}\",\n",
        "        \"Content-Type\": \"application/json\" # Adjust content type if needed\n",
        "    }\n",
        "\n",
        "    # Prepare query parameters (adjust based on API documentation)\n",
        "    params = {\n",
        "        \"q\": query\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Make the GET request to the Sonar API\n",
        "        response = requests.get(api_url, headers=headers, params=params)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Parse the JSON response\n",
        "        trends_data = response.json()\n",
        "\n",
        "        return trends_data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during Sonar API request: {e}\")\n",
        "        return None\n",
        "    except ValueError as e:\n",
        "        print(f\"Error parsing JSON response: {e}\")\n",
        "        return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3a7e432"
      },
      "source": [
        "## Implement the agent's logic\n",
        "\n",
        "### Subtask:\n",
        "Implement the core logic within the AI agent class to process user input, extract the technology trend query using the language model, call the `get_technology_trends` function, and format the response for the user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84aed8bd"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the core logic within the AI agent class to process user input, extract the technology trend query using the language model, call the `get_technology_trends` function, and format the response for the user, including the necessary error handling and data parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cf6e7c1"
      },
      "source": [
        "import openai\n",
        "# Assuming get_technology_trends is defined elsewhere and accessible\n",
        "# from .api_functions import get_technology_trends # Example if in another file\n",
        "\n",
        "class TechnologyTrendAgent:\n",
        "    \"\"\"\n",
        "    An AI agent that interacts with a language model to understand user queries about technology trends and fetches data using the Sonar API.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the AI agent with the OpenAI API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: The OpenAI API key for the language model.\n",
        "        \"\"\"\n",
        "        openai.api_key = \"YOUR-API-KEY\"\n",
        "        self.conversation_history = [] # Optional: to maintain conversation context\n",
        "\n",
        "    def process_query(self, user_input):\n",
        "        \"\"\"\n",
        "        Processes user input, extracts the technology trend query, calls the\n",
        "        Sonar API function, and returns the response.\n",
        "\n",
        "        Args:\n",
        "            user_input: The user's input string.\n",
        "\n",
        "        Returns:\n",
        "            A string containing the technology trends data or an error message.\n",
        "        \"\"\"\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        try:\n",
        "            # Use the language model to understand the user's query and extract it\n",
        "            # Using function calling for more structured extraction\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\", # Or another suitable model that supports function calling\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that identifies technology trend queries from user input.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Extract the technology trend query from the following text: '{user_input}'\"}\n",
        "                ],\n",
        "                tools=[\n",
        "                    {\n",
        "                        \"type\": \"function\",\n",
        "                        \"function\": {\n",
        "                            \"name\": \"get_technology_trends\",\n",
        "                            \"description\": \"Get the latest technology trends for a given query.\",\n",
        "                            \"parameters\": {\n",
        "                                \"type\": \"object\",\n",
        "                                \"properties\": {\n",
        "                                    \"query\": {\n",
        "                                        \"type\": \"string\",\n",
        "                                        \"description\": \"The technology trend query (e.g., 'artificial intelligence', 'blockchain').\"\n",
        "                                    }\n",
        "                                },\n",
        "                                \"required\": [\"query\"]\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                ],\n",
        "                tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_technology_trends\"}}\n",
        "            )\n",
        "\n",
        "            # Extract the function call arguments\n",
        "            tool_calls = response.choices[0].message.tool_calls\n",
        "            if tool_calls and tool_calls[0].function.name == \"get_technology_trends\":\n",
        "                import json\n",
        "                try:\n",
        "                    function_args = json.loads(tool_calls[0].function.arguments)\n",
        "                    extracted_query = function_args.get(\"query\")\n",
        "                    print(f\"Extracted query using function calling: {extracted_query}\") # For debugging\n",
        "                except json.JSONDecodeError:\n",
        "                    error_message = \"Could not parse function call arguments.\"\n",
        "                    self.conversation_history.append({\"role\": \"assistant\", \"content\": error_message})\n",
        "                    return error_message\n",
        "            else:\n",
        "                # Fallback or handle cases where function calling doesn't extract the query\n",
        "                # For simplicity, let's assume the model directly returns the query if function calling fails\n",
        "                extracted_query = response.choices[0].message.content.strip()\n",
        "                print(f\"Extracted query using direct response: {extracted_query}\") # For debugging\n",
        "\n",
        "\n",
        "            if not extracted_query:\n",
        "                 error_message = \"Could not extract a technology trend query from your request.\"\n",
        "                 self.conversation_history.append({\"role\": \"assistant\", \"content\": error_message})\n",
        "                 return error_message\n",
        "\n",
        "\n",
        "            # Call the Sonar API function with the extracted query\n",
        "            trends_data = get_technology_trends(extracted_query)\n",
        "\n",
        "            if trends_data:\n",
        "                # Process and format the trends_data for the user\n",
        "                formatted_response = f\"Here are the technology trends for '{extracted_query}':\\n\"\n",
        "                # Assuming trends_data is a dictionary with a 'trends' key containing a list of trend dictionaries\n",
        "                if isinstance(trends_data, dict) and 'trends' in trends_data and isinstance(trends_data['trends'], list):\n",
        "                    if trends_data['trends']:\n",
        "                        for trend in trends_data['trends']:\n",
        "                            # Adjust key names based on actual Sonar API response structure\n",
        "                            name = trend.get('name', 'N/A')\n",
        "                            description = trend.get('description', 'No description available.')\n",
        "                            formatted_response += f\"- **{name}**: {description}\\n\"\n",
        "                    else:\n",
        "                        formatted_response += \"No specific trends found.\"\n",
        "                else:\n",
        "                    formatted_response += \"Could not retrieve detailed trend information or the data format was unexpected.\"\n",
        "\n",
        "\n",
        "                self.conversation_history.append({\"role\": \"assistant\", \"content\": formatted_response})\n",
        "                return formatted_response\n",
        "            else:\n",
        "                error_message = f\"Could not retrieve technology trends for '{extracted_query}'. The Sonar API might have returned an error or no data.\"\n",
        "                self.conversation_history.append({\"role\": \"assistant\", \"content\": error_message})\n",
        "                return error_message\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"An error occurred while processing your request: {e}\"\n",
        "            self.conversation_history.append({\"role\": \"assistant\", \"content\": error_message})\n",
        "            return error_message\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4508e6e8"
      },
      "source": [
        "## Test the agent\n",
        "\n",
        "### Subtask:\n",
        "Write test cases to ensure the agent correctly retrieves and presents technology trends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c72df87"
      },
      "source": [
        "**Reasoning**:\n",
        "Write test cases for the TechnologyTrendAgent using unittest and mocking.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2db988a",
        "outputId": "b77d998d-6ec7-4b93-f0bb-d4839f61e953"
      },
      "source": [
        "agent = TechnologyTrendAgent()\n",
        "output = agent.process_query(\"What is the most common ai agent framework right now?\")\n",
        "\n",
        "print(output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted query using function calling: ai agent framework\n",
            "Error during Sonar API request: HTTPSConnectionPool(host='api.getsonar.com', port=443): Max retries exceeded with url: /v1/trends?q=ai+agent+framework (Caused by SSLError(SSLError(1, '[SSL: TLSV1_UNRECOGNIZED_NAME] tlsv1 unrecognized name (_ssl.c:1010)')))\n",
            "Could not retrieve technology trends for 'ai agent framework'. The Sonar API might have returned an error or no data.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2987b129c5d644df94c4d2c4b8eb8abc": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1092991dba0c40acbdc1a5fb8e0573fb",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Evaluating 1 test case(s) in parallel \u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[38;2;0;229;255m  0%\u001b[0m \u001b[38;2;87;3;255m0:00:08\u001b[0m\n    ğŸ¯ Evaluating test case #0        \u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[38;2;0;229;255m  0%\u001b[0m \u001b[38;2;87;3;255m0:00:08\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating 1 test case(s) in parallel <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #00e5ff; text-decoration-color: #00e5ff\">  0%</span> <span style=\"color: #5703ff; text-decoration-color: #5703ff\">0:00:08</span>\n    ğŸ¯ Evaluating test case #0        <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #00e5ff; text-decoration-color: #00e5ff\">  0%</span> <span style=\"color: #5703ff; text-decoration-color: #5703ff\">0:00:08</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "1092991dba0c40acbdc1a5fb8e0573fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}